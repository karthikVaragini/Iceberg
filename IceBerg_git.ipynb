{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b988c259-6080-4ec8-8ad2-35d75c583e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeltaTable loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import findspark\n",
    "from delta.tables import DeltaTable\n",
    "print(\"DeltaTable loaded:\", bool(DeltaTable))\n",
    "\n",
    "# Initialize Spark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, TimestampType, LongType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import coalesce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c2ca48-4413-4e16-936e-17ebf17dbac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_jar_path = \"/home/karthik/iceberg-spark-runtime-3.4_2.12-1.4.2.jar\"\n",
    "hive_jdbc = \"/opt/spark/jars/hive-jdbc-2.3.9.jar\"\n",
    "\n",
    "spark_access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "spark_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0dfb41-ef99-4cf2-bcd3-2f9eadd6b503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.4.0\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with Iceberg configurations\n",
    "spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergToS3WithHiveSync\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.jars\", f\"{iceberg_jar_path}\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.uri\", \"thrift://XXXX.XX.XX.XX:32431\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"XXXXXX\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"XXXXXXXXX\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.uri\", \"thrift://XX.XX.XX.XX:32431\") \\\n",
    "    .config(\"spark.sql.catalog.my_catalog.warehouse\", \"s3a://XXXX/iceberg/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "# Check the Spark version to confirm setup\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48d7099b-abde-4d09-9e60-72334883f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current schema: default\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/03 18:33:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify the current schema\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "print(f\"Current schema: {current_schema}\")\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer\", StringType(), False),\n",
    "    StructField(\"user_id\", StringType(), False),\n",
    "     StructField(\"picture_url\", StringType(), True),\n",
    "    StructField(\"user_type\", StringType(), True),\n",
    "    StructField(\"created_date\", StringType(), True),\n",
    "    StructField(\"audit_timestamp\", LongType(), False)\n",
    "])\n",
    "\n",
    "# Read data from a CSV file into a DataFrame\n",
    "df = spark.read.schema(schema).json(\"s3a://XXXX/data/customers/4e0333a1205e_308_2_2.json\")\n",
    "df.printSchema()\n",
    "#We can select the required columns only\n",
    "selected_columns_df = df.select(\"customer\", \"user_id\", \"picture_url\", \"user_type\", \"audit_timestamp\") \n",
    "\n",
    "\n",
    "# Check if there are any null values in customer or user_id\n",
    "null_counts = selected_columns_df.filter(col(\"customer\").isNull() | col(\"user_id\").isNull() | col(\"audit_timestamp\").isNull()).count() \n",
    "\n",
    "if null_counts == 0:\n",
    "    # Create DataFrame with the new schema\n",
    "    df_new = spark.createDataFrame(selected_columns_df.rdd, schema=schema)\n",
    "else:\n",
    "    print(\"Data contains null values in 'customer' or 'user_id'. Handling nulls before applying the new schema.\")\n",
    "    # Handle null values by filling with default values\n",
    "    df_handled = selected_columns_df.na.fill({\n",
    "        'customer': 'default_customer',  # Replace with an appropriate default value\n",
    "        'user_id': 'default_user_id',\n",
    "        'audit_timestamp': 0 # Replace with an appropriate default value\n",
    "    })\n",
    "    # Create DataFrame with the new schema\n",
    "    df_new = spark.createDataFrame(df_handled.rdd, schema=schema)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a4a514-306a-48e2-b64b-759a1473b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema to validate the Schema\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb5e803-b5a0-4508-8eda-9a26684a2288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to Iceberg table successfully\n"
     ]
    }
   ],
   "source": [
    "table_name = \"default.customers\"\n",
    "# Check if the table exists, if not, create it\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS  {table_name} (\n",
    "    customer STRING,\n",
    "    user_id STRING NOT NULL,\n",
    "    user_type STRING,\n",
    "    audit_timestamp bigint\n",
    "    \n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (customer)\n",
    " LOCATION 's3a://XXXX/iceberg/warehouse/customers/'\n",
    " \"\"\")\n",
    "# #Adding New columns\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name} \n",
    "ADD COLUMNS picture_url STRING\n",
    "\"\"\")\n",
    "# #Adding Identifier, Identifier column should ne NOT NULL type\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {table_name} SET IDENTIFIER FIELDS user_id\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"   \n",
    "ALTER TABLE {table_name} SET \n",
    "TBLPROPERTIES ('write.update.mode'='copy-on-write',\n",
    "    'write.update.isolation-level'='snapshot',\n",
    "    'write.merge.mode'='copy-on-write',\n",
    "    'write.merge.isolation-level'='snapshot',\n",
    "    'write.target-file-size-bytes'='100870912',\n",
    "    'write.distribution.mode'='hash')\n",
    "\"\"\")\n",
    "\n",
    "# # Write data to Iceberg table\n",
    "selected_columns_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(table_name)\n",
    "\n",
    "print(\"Data written to Iceberg table successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
